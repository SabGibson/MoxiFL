{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29de1de4",
   "metadata": {},
   "source": [
    "# Experiment 3: Additional Labels Experiments\n",
    "This notebook contains templates for additional labels experiments with different configurations:\n",
    "- Tasks: Binary Classification and Image Classification (no regression for label experiments)\n",
    "- Network Types: Centralised and Decentralised\n",
    "- Network Size: 10 (fixed)\n",
    "- LabelPartitioner: 10 (fixed)\n",
    "- Additional Labels: 1, 3, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c6edf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgtec/miniconda3/envs/moxifl/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "from flwr_datasets.partitioner import DirichletPartitioner, Partitioner\n",
    "from moxi import create_experiment\n",
    "\n",
    "# Initialize asyncio for Jupyter\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294376ca",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "Define the models for classification tasks (binary and image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04e6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification Model\n",
    "class BinaryClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(30, 64)\n",
    "        self.act_func = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act_func(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Image Classification Model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.maxpool(self.conv1(x)))\n",
    "        x = self.activation(self.maxpool(self.conv2(x)))\n",
    "        x = self.activation(self.maxpool(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd4a0f",
   "metadata": {},
   "source": [
    "## Experiment Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6131439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_experiment(flnw, num_rounds:int, epochs_per_round:int, experiment_name:str):\n",
    "    \"\"\"\n",
    "    Execute a federated learning experiment.\n",
    "    \n",
    "    Args:\n",
    "        flnw: Federated learning network object\n",
    "        num_rounds: Number of federated learning rounds\n",
    "        epochs_per_round: Number of local epochs per round\n",
    "        experiment_name: Name for the experiment\n",
    "    \"\"\"\n",
    "    await flnw.train(num_rounds=num_rounds, epochs_per_round=epochs_per_round, experiment_name=experiment_name)\n",
    "    print(f\"Experiment '{experiment_name}' completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8632a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f43b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "from flwr_datasets.partitioner import Partitioner\n",
    "\n",
    "class LabelDirichletPartitioner(Partitioner):\n",
    "    \"\"\"Filter by labels, then distribute with Dirichlet across clients.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_labels=None,\n",
    "        test_labels=None,\n",
    "        alpha: float = 0.5,\n",
    "        seed: int = 42,\n",
    "        num_partitions: int = 1,\n",
    "        label_key: str = \"label\",  # dataset column to partition by\n",
    "    ):\n",
    "        # Initialize the parent class (sets _dataset = None)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.train_labels = train_labels\n",
    "        self.test_labels = test_labels\n",
    "        self.alpha = alpha\n",
    "        self.seed = seed\n",
    "        self._num_partitions = num_partitions\n",
    "        self.label_key = label_key\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        # Cache for partition indices (computed once when dataset is assigned)\n",
    "        self._partition_indices = None\n",
    "        self._filtered_dataset = None\n",
    "\n",
    "    @property\n",
    "    def num_partitions(self) -> int:\n",
    "        return self._num_partitions\n",
    "\n",
    "    @property\n",
    "    def dataset(self) -> Dataset:\n",
    "        \"\"\"Dataset property (inherited from Partitioner).\"\"\"\n",
    "        return super().dataset\n",
    "\n",
    "    @dataset.setter\n",
    "    def dataset(self, value: Dataset) -> None:\n",
    "        \"\"\"Set the dataset and compute partitions.\"\"\"\n",
    "        # Use parent setter (includes validation)\n",
    "        super(LabelDirichletPartitioner, self.__class__).dataset.fset(self, value)\n",
    "        \n",
    "        # Apply label filtering if specified\n",
    "        if self.train_labels is not None:\n",
    "            self._filtered_dataset = value.filter(\n",
    "                lambda x: x[self.label_key] in self.train_labels\n",
    "            )\n",
    "        else:\n",
    "            self._filtered_dataset = value\n",
    "            \n",
    "        # Compute partition indices once when dataset is assigned\n",
    "        labels = np.array(self._filtered_dataset[self.label_key])\n",
    "        self._partition_indices = self._dirichlet_split(labels, self._num_partitions)\n",
    "\n",
    "    def load_partition(self, partition_id: int) -> Dataset:\n",
    "        \"\"\"Load a single partition based on the partition index.\"\"\"\n",
    "        if not self.is_dataset_assigned():\n",
    "            raise ValueError(\"Dataset must be assigned before loading partitions\")\n",
    "            \n",
    "        if partition_id >= self._num_partitions:\n",
    "            raise ValueError(f\"partition_id {partition_id} >= num_partitions {self._num_partitions}\")\n",
    "        \n",
    "        if partition_id < 0:\n",
    "            raise ValueError(f\"partition_id must be non-negative, got {partition_id}\")\n",
    "            \n",
    "        # Return the partition using precomputed indices\n",
    "        partition_indices = self._partition_indices[partition_id]\n",
    "        if len(partition_indices) == 0:\n",
    "            # Return empty dataset with same structure if no samples for this partition\n",
    "            return self._filtered_dataset.select([])\n",
    "            \n",
    "        return self._filtered_dataset.select(partition_indices)\n",
    "\n",
    "    def _dirichlet_split(self, labels, num_partitions):\n",
    "        \"\"\"Split indices into clients using a Dirichlet distribution per class.\"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return [[] for _ in range(num_partitions)]\n",
    "            \n",
    "        unique_labels = np.unique(labels)\n",
    "        label_indices = {c: np.where(labels == c)[0] for c in unique_labels}\n",
    "        client_indices = [[] for _ in range(num_partitions)]\n",
    "\n",
    "        for c, idxs in label_indices.items():\n",
    "            if len(idxs) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Generate proportions per client for this class\n",
    "            proportions = self.rng.dirichlet([self.alpha] * num_partitions)\n",
    "            \n",
    "            # Shuffle indices to ensure random distribution\n",
    "            self.rng.shuffle(idxs)\n",
    "            \n",
    "            # Calculate how many samples each client gets for this class\n",
    "            counts = (proportions * len(idxs)).astype(int)\n",
    "            \n",
    "            # Handle rounding issues - distribute remaining samples\n",
    "            remaining = len(idxs) - counts.sum()\n",
    "            for i in range(remaining):\n",
    "                counts[i % num_partitions] += 1\n",
    "            \n",
    "            # Distribute indices to clients\n",
    "            start_idx = 0\n",
    "            for client_id, count in enumerate(counts):\n",
    "                if count > 0:\n",
    "                    end_idx = start_idx + count\n",
    "                    client_indices[client_id].extend(idxs[start_idx:end_idx].tolist())\n",
    "                    start_idx = end_idx\n",
    "\n",
    "        return client_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689b49f6",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ee45bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "additional_labels = [1, 3, 5]  # Additional labels to test\n",
    "ADDITIONAL_LABEL_TRAIN = {1: [0,1], 3: [0,1,2,3], 5: [0,1,2,3,4,5]}\n",
    "testing_labels = [1] \n",
    "network_types = [\"centralised\", \"decentralised\"]  # Network architectures\n",
    "task_types = [\n",
    "    {\"name\": \"image_classification\", \"model\": CNNModel, \"criterion\": nn.CrossEntropyLoss}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc9a9f",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "Choose a specific configuration to run by uncommenting and modifying the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3226b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntask_type = task_types[0]  # 0: binary_classification, 1: image_classification\\nnetwork_type = network_types[0]  # 0: centralised, 1: decentralised\\nadditional_label = additional_labels[0]  # Choose from additional_labels list\\n\\n# Create the configuration\\nconfig = {\\n    \"network_name\": f\"{network_type}_{task_type[\\'name\\']}_addlabels_{additional_label}\",\\n    \"network_type\": network_type,\\n    \"model_type\": \"parametric\",\\n    \"ml_framework\": \"pytorch\",\\n    \"comms\": \"async\",\\n    \"federated_rounds\": 5,\\n    \"network_size\": 10,  # Fixed network size for Experiment 3\\n    \"metrics\": [\"mean_perfomance\", \"convergence\"],\\n    \"logger\": \"mlflow\",\\n    \"node_base_config\": {\\n        \"model\": None,\\n        \"learning_rate\": 1e-3,\\n        \"optimizer\": SGD,\\n        \"train_data\": None,\\n        \"val_data\": None,\\n        \"criterion\": task_type[\"criterion\"],\\n        \"random_sampling\": False,\\n        \"n_epochs\": 5,\\n        \"batch_size\": 16\\n    },\\n    \"experiment_config\": {\\n        \"task\": task_type[\"name\"],\\n        \"partitioner\": LabelDirichletPartitioner,\\n        \"partitioner_params\": { \"train_labels\": ADDITIONAL_LABEL_TRAIN[additional_label], \"test_labels\": testing_labels, \"alpha\":10 }, # Fixed alpha for Experiment 3\\n        \"num_worker\": 2,\\n        \"max_samples\": 1000,\\n        \"additional_labels\": additional_label  # Variable parameter\\n    },\\n    \"adjcency_matrix\": None,\\n    \"model\": None\\n}\\n\\n# Create the model\\nmodel = task_type[\"model\"]()\\n\\n# Create the experiment\\nprint(f\"Running experiment: {network_type} {task_type[\\'name\\']} with {additional_label} additional labels\")\\nflnw = create_experiment(config, model)\\n\\n# Run the experiment\\nawait execute_experiment(\\n    flnw, \\n    num_rounds=5,  # Set to desired number (20 for full experiment)\\n    epochs_per_round=1, \\n    experiment_name=f\"exp3_{network_type}_{task_type[\\'name\\']}_addlabels_{additional_label}\"\\n)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Centralised Binary Classification with 1 additional label\n",
    "# Change these variables to run different experiments\n",
    "\"\"\"\n",
    "task_type = task_types[0]  # 0: binary_classification, 1: image_classification\n",
    "network_type = network_types[0]  # 0: centralised, 1: decentralised\n",
    "additional_label = additional_labels[0]  # Choose from additional_labels list\n",
    "\n",
    "# Create the configuration\n",
    "config = {\n",
    "    \"network_name\": f\"{network_type}_{task_type['name']}_addlabels_{additional_label}\",\n",
    "    \"network_type\": network_type,\n",
    "    \"model_type\": \"parametric\",\n",
    "    \"ml_framework\": \"pytorch\",\n",
    "    \"comms\": \"async\",\n",
    "    \"federated_rounds\": 5,\n",
    "    \"network_size\": 10,  # Fixed network size for Experiment 3\n",
    "    \"metrics\": [\"mean_perfomance\", \"convergence\"],\n",
    "    \"logger\": \"mlflow\",\n",
    "    \"node_base_config\": {\n",
    "        \"model\": None,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"optimizer\": SGD,\n",
    "        \"train_data\": None,\n",
    "        \"val_data\": None,\n",
    "        \"criterion\": task_type[\"criterion\"],\n",
    "        \"random_sampling\": False,\n",
    "        \"n_epochs\": 5,\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    \"experiment_config\": {\n",
    "        \"task\": task_type[\"name\"],\n",
    "        \"partitioner\": LabelDirichletPartitioner,\n",
    "        \"partitioner_params\": { \"train_labels\": ADDITIONAL_LABEL_TRAIN[additional_label], \"test_labels\": testing_labels, \"alpha\":10 }, # Fixed alpha for Experiment 3\n",
    "        \"num_worker\": 2,\n",
    "        \"max_samples\": 1000,\n",
    "        \"additional_labels\": additional_label  # Variable parameter\n",
    "    },\n",
    "    \"adjcency_matrix\": None,\n",
    "    \"model\": None\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = task_type[\"model\"]()\n",
    "\n",
    "# Create the experiment\n",
    "print(f\"Running experiment: {network_type} {task_type['name']} with {additional_label} additional labels\")\n",
    "flnw = create_experiment(config, model)\n",
    "\n",
    "# Run the experiment\n",
    "await execute_experiment(\n",
    "    flnw, \n",
    "    num_rounds=5,  # Set to desired number (20 for full experiment)\n",
    "    epochs_per_round=1, \n",
    "    experiment_name=f\"exp3_{network_type}_{task_type['name']}_addlabels_{additional_label}\"\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3834ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom experiment runner - select specific combinations\n",
    "# Uncomment and modify to run specific experiment combinations\n",
    "async def run_selected_experiments(selected_task_types=None, selected_network_types=None, selected_additional_labels=None):\n",
    "    \"\"\"\n",
    "    Run experiments with specific parameter combinations\n",
    "    \n",
    "    Args:\n",
    "        selected_task_types: List of task indices to run (e.g., [0] for binary_classification only)\n",
    "        selected_network_types: List of network type indices to run (e.g., [0] for centralised only)\n",
    "        selected_additional_labels: List of additional label values to run (e.g., [1, 3])\n",
    "    \"\"\"\n",
    "    if selected_task_types is None:\n",
    "        selected_task_types = range(len(task_types))\n",
    "    if selected_network_types is None:\n",
    "        selected_network_types = range(len(network_types))\n",
    "    if selected_additional_labels is None:\n",
    "        selected_additional_labels = additional_labels\n",
    "        \n",
    "    experiment_count = 0\n",
    "    total_experiments = len(selected_task_types) * len(selected_network_types) * len(selected_additional_labels)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting selected experiments: {total_experiments} total experiments to run\")\n",
    "    \n",
    "    # Loop through selected task types\n",
    "    for task_idx in selected_task_types:\n",
    "        task_type = task_types[task_idx]\n",
    "        \n",
    "        # Loop through selected network types\n",
    "        for net_idx in selected_network_types:\n",
    "            network_type = network_types[net_idx]\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Running experiments for {network_type} {task_type['name']}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Loop through selected additional label values\n",
    "            for additional_label in selected_additional_labels:\n",
    "                experiment_count += 1\n",
    "                \n",
    "                print(f\"\\nExperiment {experiment_count}/{total_experiments}\")\n",
    "                print(f\"Task: {task_type['name']}, Network: {network_type}, Additional Labels: {additional_label}\")\n",
    "                \n",
    "                # Create configuration\n",
    "                config = {\n",
    "                    \"network_name\": f\"{network_type}_{task_type['name']}_addlabels_{additional_label}\",\n",
    "                    \"network_type\": network_type,\n",
    "                    \"model_type\": \"parametric\",\n",
    "                    \"ml_framework\": \"pytorch\",\n",
    "                    \"comms\": \"async\",\n",
    "                    \"federated_rounds\": 5,\n",
    "                    \"network_size\": 10,  # Fixed for Experiment 3\n",
    "                    \"metrics\": [\"mean_perfomance\", \"convergence\"],\n",
    "                    \"logger\": \"mlflow\",\n",
    "                    \"node_base_config\": {\n",
    "                        \"model\": None,\n",
    "                        \"learning_rate\": 1e-3,\n",
    "                        \"optimizer\": SGD,\n",
    "                        \"train_data\": None,\n",
    "                        \"val_data\": None,\n",
    "                        \"criterion\": task_type[\"criterion\"],\n",
    "                        \"random_sampling\": False,\n",
    "                        \"n_epochs\": 5,\n",
    "                        \"batch_size\": 16\n",
    "                    },\n",
    "                    \"experiment_config\": {\n",
    "                        \"task\": task_type[\"name\"],\n",
    "                        \"partitioner\": DirichletPartitioner,\n",
    "                        \"alpha\": 10,  # Fixed alpha for Experiment 3\n",
    "                        \"num_worker\": 2,\n",
    "                        \"max_samples\": 1000,\n",
    "                        \"additional_labels\": additional_label  # Variable parameter\n",
    "                    },\n",
    "                    \"adjcency_matrix\": None,\n",
    "                    \"model\": None\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    # Create model\n",
    "                    model = task_type[\"model\"]()\n",
    "                    \n",
    "                    # Create experiment\n",
    "                    print(f\"Creating experiment: {network_type} {task_type['name']} with {additional_label} additional labels\")\n",
    "                    flnw = create_experiment(config, model)\n",
    "                    \n",
    "                    # Execute experiment\n",
    "                    print(f\"Running experiment...\")\n",
    "                    await execute_experiment(\n",
    "                        flnw, \n",
    "                        num_rounds=5,  # Set to 20 for full experiment, 5 for testing\n",
    "                        epochs_per_round=1,\n",
    "                        experiment_name=f\"exp3_{network_type}_{task_type['name']}_addlabels_{additional_label}\"\n",
    "                    )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in experiment {experiment_count}: {e}\")\n",
    "                \n",
    "                # Wait briefly between experiments\n",
    "                await asyncio.sleep(2)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Selected experiments completed!\")\n",
    "    print(f\"Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "# Run only binary classification with both network types but only additional labels 1 and 3\n",
    "# await run_selected_experiments(\n",
    "#     selected_task_types=[0],  # 0: binary_classification\n",
    "#     selected_network_types=[0, 1],  # Both centralised and decentralised\n",
    "#     selected_additional_labels=[1, 3]  # Only fewer additional labels\n",
    "# )\n",
    "\n",
    "# Run all tasks but only for centralised networks with 1 additional label\n",
    "# await run_selected_experiments(\n",
    "#     selected_network_types=[0],  # 0: centralised\n",
    "#     selected_additional_labels=[1]  # Only 1 additional label\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ccc27",
   "metadata": {},
   "source": [
    "## Automated Experiment Loop\n",
    "This cell runs all experiments for a specific network type and task type.\n",
    "Uncomment and modify to run a batch of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94e2d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all experiments for all task types and network types\n",
    "import asyncio\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "async def run_all_experiments():\n",
    "    \"\"\"Run all additional labels experiments for all task types and network architectures\"\"\"\n",
    "    \n",
    "    experiment_count = 0\n",
    "    total_experiments = len(task_types) * len(network_types) * len(additional_labels)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Starting all experiments: {total_experiments} total experiments to run\")\n",
    "    print(f\"Start time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Loop through all task types\n",
    "    for task_idx, task_type in enumerate(task_types):\n",
    "        # Loop through all network types\n",
    "        for net_idx, network_type in enumerate(network_types):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Running experiments for {network_type} {task_type['name']}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Loop through all additional label values\n",
    "            for label_idx, additional_label in enumerate(additional_labels):\n",
    "                experiment_count += 1\n",
    "                \n",
    "                print(f\"\\nExperiment {experiment_count}/{total_experiments}\")\n",
    "                print(f\"Task: {task_type['name']}, Network: {network_type}, Additional Labels: {additional_label}\")\n",
    "                \n",
    "                # Create configuration\n",
    "                config = {\n",
    "                    \"network_name\": f\"{network_type}_{task_type['name']}_addlabels_{additional_label}\",\n",
    "                    \"network_type\": network_type,\n",
    "                    \"model_type\": \"parametric\",\n",
    "                    \"ml_framework\": \"pytorch\",\n",
    "                    \"comms\": \"async\",\n",
    "                    \"federated_rounds\": 5,\n",
    "                    \"network_size\": 10,  # Fixed for Experiment 3\n",
    "                    \"metrics\": [\"mean_perfomance\", \"convergence\"],\n",
    "                    \"logger\": \"mlflow\",\n",
    "                    \"node_base_config\": {\n",
    "                        \"model\": None,\n",
    "                        \"learning_rate\": 1e-3,\n",
    "                        \"optimizer\": SGD,\n",
    "                        \"train_data\": None,\n",
    "                        \"val_data\": None,\n",
    "                        \"criterion\": task_type[\"criterion\"],\n",
    "                        \"random_sampling\": False,\n",
    "                        \"n_epochs\": 5,\n",
    "                        \"batch_size\": 16\n",
    "                    },\n",
    "                    \"experiment_config\": {\n",
    "                        \"task\": task_type[\"name\"],\n",
    "                        \"partitioner\": LabelDirichletPartitioner,\n",
    "                        \"partitioner_params\": { \"train_labels\": ADDITIONAL_LABEL_TRAIN[additional_label], \"test_labels\": testing_labels, \"alpha\":10, \"num_partitions\":10 }, # Fixed alpha for Experiment 3\n",
    "                        \"num_worker\": 2,\n",
    "                        \"max_samples\": 1000,\n",
    "                        \"additional_labels\": additional_label  # Variable parameter\n",
    "                    },\n",
    "                    \"adjcency_matrix\": None,\n",
    "                    \"model\": None\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    # Create model\n",
    "                    model = task_type[\"model\"]()\n",
    "                    \n",
    "                    # Create experiment\n",
    "                    print(f\"Creating experiment: {network_type} {task_type['name']} with {additional_label} additional labels\")\n",
    "                    flnw = create_experiment(config, model)\n",
    "                    \n",
    "                    # Execute experiment\n",
    "                    print(f\"Running experiment...\")\n",
    "                    await execute_experiment(\n",
    "                        flnw, \n",
    "                        num_rounds=5,  # Set to 20 for full experiment, 5 for testing\n",
    "                        epochs_per_round=1,\n",
    "                        experiment_name=f\"exp3_{network_type}_{task_type['name']}_addlabels_{additional_label}\"\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate elapsed time\n",
    "                    elapsed = time.time() - start_time\n",
    "                    avg_time_per_exp = elapsed / experiment_count\n",
    "                    remaining = avg_time_per_exp * (total_experiments - experiment_count)\n",
    "                    \n",
    "                    # Print progress\n",
    "                    print(f\"\\nCompleted experiment {experiment_count}/{total_experiments}\")\n",
    "                    print(f\"Elapsed time: {elapsed:.1f} seconds\")\n",
    "                    print(f\"Estimated time remaining: {remaining:.1f} seconds\")\n",
    "                    print(f\"Estimated completion time: {datetime.datetime.now() + datetime.timedelta(seconds=remaining)}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in experiment {experiment_count}: {e}\")\n",
    "                \n",
    "                # Wait briefly between experiments\n",
    "                await asyncio.sleep(2)\n",
    "    \n",
    "    # Print completion summary\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"All experiments completed!\")\n",
    "    print(f\"Total time: {total_time:.1f} seconds\")\n",
    "    print(f\"End time: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90021768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting all experiments: 6 total experiments to run\n",
      "Start time: 2025-09-09 09:41:58\n",
      "\n",
      "================================================================================\n",
      "Running experiments for centralised image_classification\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Experiment 1/6\n",
      "Task: image_classification, Network: centralised, Additional Labels: 1\n",
      "Creating experiment: centralised image_classification with 1 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:42:01 INFO mlflow.tracking.fluent: Experiment with name 'exp3_centralised_image_classification_addlabels_1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_centralised_image_classification_addlabels_1' completed.\n",
      "\n",
      "Completed experiment 1/6\n",
      "Elapsed time: 99.7 seconds\n",
      "Estimated time remaining: 498.7 seconds\n",
      "Estimated completion time: 2025-09-09 09:51:56.559776\n",
      "\n",
      "Experiment 2/6\n",
      "Task: image_classification, Network: centralised, Additional Labels: 3\n",
      "Creating experiment: centralised image_classification with 3 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2717209e0449879d4701c3e6d9ce7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:43:48 INFO mlflow.tracking.fluent: Experiment with name 'exp3_centralised_image_classification_addlabels_3' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_centralised_image_classification_addlabels_3' completed.\n",
      "\n",
      "Completed experiment 2/6\n",
      "Elapsed time: 212.8 seconds\n",
      "Estimated time remaining: 425.5 seconds\n",
      "Estimated completion time: 2025-09-09 09:52:36.468043\n",
      "\n",
      "Experiment 3/6\n",
      "Task: image_classification, Network: centralised, Additional Labels: 5\n",
      "Creating experiment: centralised image_classification with 5 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3692f24c622407dbbceeeef97ee12c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:45:41 INFO mlflow.tracking.fluent: Experiment with name 'exp3_centralised_image_classification_addlabels_5' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_centralised_image_classification_addlabels_5' completed.\n",
      "\n",
      "Completed experiment 3/6\n",
      "Elapsed time: 326.6 seconds\n",
      "Estimated time remaining: 326.6 seconds\n",
      "Estimated completion time: 2025-09-09 09:52:51.395277\n",
      "\n",
      "================================================================================\n",
      "Running experiments for decentralised image_classification\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Experiment 4/6\n",
      "Task: image_classification, Network: decentralised, Additional Labels: 1\n",
      "Creating experiment: decentralised image_classification with 1 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:47:28 INFO mlflow.tracking.fluent: Experiment with name 'exp3_decentralised_image_classification_addlabels_1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_decentralised_image_classification_addlabels_1' completed.\n",
      "\n",
      "Completed experiment 4/6\n",
      "Elapsed time: 428.3 seconds\n",
      "Estimated time remaining: 214.2 seconds\n",
      "Estimated completion time: 2025-09-09 09:52:40.666476\n",
      "\n",
      "Experiment 5/6\n",
      "Task: image_classification, Network: decentralised, Additional Labels: 3\n",
      "Creating experiment: decentralised image_classification with 3 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:49:10 INFO mlflow.tracking.fluent: Experiment with name 'exp3_decentralised_image_classification_addlabels_3' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_decentralised_image_classification_addlabels_3' completed.\n",
      "\n",
      "Completed experiment 5/6\n",
      "Elapsed time: 535.4 seconds\n",
      "Estimated time remaining: 107.1 seconds\n",
      "Estimated completion time: 2025-09-09 09:52:40.642968\n",
      "\n",
      "Experiment 6/6\n",
      "Task: image_classification, Network: decentralised, Additional Labels: 5\n",
      "Creating experiment: decentralised image_classification with 5 additional labels\n",
      "Loading federated CIFAR-10 dataset: uoft-cs/cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/09 09:50:57 INFO mlflow.tracking.fluent: Experiment with name 'exp3_decentralised_image_classification_addlabels_5' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment...\n",
      "Training Complete!\n",
      "Experiment 'exp3_decentralised_image_classification_addlabels_5' completed.\n",
      "\n",
      "Completed experiment 6/6\n",
      "Elapsed time: 641.3 seconds\n",
      "Estimated time remaining: 0.0 seconds\n",
      "Estimated completion time: 2025-09-09 09:52:39.443065\n",
      "\n",
      "================================================================================\n",
      "All experiments completed!\n",
      "Total time: 643.3 seconds\n",
      "End time: 2025-09-09 09:52:41\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await run_all_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667d209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moxifl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
